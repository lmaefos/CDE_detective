{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2621a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from Levenshtein import distance as lev_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae70cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"Normalize string by converting to lowercase, handling NaN or float, and removing non-alphanumeric characters.\"\"\"\n",
    "    if isinstance(s, str):  # Only process strings\n",
    "        return re.sub(r'\\W+', '', s.lower())\n",
    "    else:\n",
    "        return ''  # Or you can return 'Missing Data' if you'd prefer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0e4a9",
   "metadata": {},
   "source": [
    "### String Normalization Utility\n",
    "\n",
    "This script sets up basic tools for data cleaning:\n",
    "- **pandas** for handling datasets\n",
    "- **re** for regular expression-based text cleaning\n",
    "- **Levenshtein distance** for fuzzy string comparison\n",
    "\n",
    "It includes a helper function `normalize_string()` that:\n",
    "- Lowercases text\n",
    "- Removes non-alphanumeric characters\n",
    "- Handles missing values gracefully\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd4732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_to_percentage(str1, str2):\n",
    "    \"\"\"\n",
    "    Convert Levenshtein distance to a percentage similarity score.\n",
    "    100% means an exact match, 0% means completely different.\n",
    "    \"\"\"\n",
    "    lev_dist = lev_distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return (1 - lev_dist / max_len) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3c15f",
   "metadata": {},
   "source": [
    "### Levenshtein Distance to Percentage Similarity\n",
    "\n",
    "This helper function `levenshtein_to_percentage()` compares two strings and returns a **similarity score** expressed as a percentage:\n",
    "- **100%** → identical strings\n",
    "- **0%** → completely different strings\n",
    "\n",
    "It calculates the number of edits needed to transform one string into the other (using Levenshtein distance) and then scales that difference relative to the length of the longer string.\n",
    "\n",
    "Example:\n",
    "- `\"PainScore\"` vs. `\"Pain_Score\"` ➔ 90% similar\n",
    "- `\"Pain\"` vs. `\"Pain\"` ➔ 100% similar\n",
    "- `\"Pain\"` vs. `\"Gain\"` ➔ 75% similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c911c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_encodings(study_file, encoding_column='encodings', field_label_column='field_label', cde_file='./KnowledgeBase/Compiled_CORE_CDEs list_English_one sheet_as of 2025-01-28.xlsx', study_sheet='Sheet1'):\n",
    "    \"\"\"\n",
    "    Compare study data dictionary encodings and field labels with HEAL CDE encodings using Levenshtein distance.\n",
    "    \n",
    "    Parameters:\n",
    "    - study_file: Path to the study data dictionary file (.xlsx or .csv).\n",
    "    - encoding_column: Name of the column containing encodings in the study file.\n",
    "    - field_label_column: Name of the column containing field labels (questions) in the study file.\n",
    "    - cde_file: Path to the HEAL CDE knowledge base file (.xlsx).\n",
    "    - study_sheet: Name of the sheet in the study file to process (default is 'Sheet1').\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Original study data with match results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load study data from the specified sheet\n",
    "    if study_file.endswith('.xlsx'):\n",
    "        study_df = pd.read_excel(study_file, sheet_name=study_sheet)\n",
    "    else:\n",
    "        study_df = pd.read_csv(study_file)\n",
    "    \n",
    "    # Initialize new columns for matches\n",
    "    study_df['Best Match CDE Name'] = None\n",
    "    study_df['Best Match Score'] = None\n",
    "    study_df['Best Match CRF Name'] = None\n",
    "        \n",
    "    study_df['Potential Match 2 - CDE Name'] = None\n",
    "    study_df['Potential Match 2 - Score'] = None\n",
    "    study_df['Potential Match 2 - CRF Name'] = None\n",
    "\n",
    "    study_df['Potential Match 3 - CDE Name'] = None\n",
    "    study_df['Potential Match 3 - Score'] = None\n",
    "    study_df['Potential Match 3 - CRF Name'] = None\n",
    "\n",
    "    # Skip rows where either encoding or field label is blank or NaN\n",
    "    study_df['Normalized Combined'] = study_df.apply(\n",
    "        lambda row: normalize_string(row[encoding_column] + \" | \" + row[field_label_column]) \n",
    "        if pd.notna(row[encoding_column]) and pd.notna(row[field_label_column]) else '', axis=1\n",
    "    )\n",
    "\n",
    "    # Load HEAL CDE encodings and field labels\n",
    "    cde_df = pd.read_excel(cde_file, sheet_name='ALL')\n",
    "    cde_df = cde_df.dropna(subset=['PV Description', 'Additional Notes (Question Text)'])\n",
    "    \n",
    "    # Concatenate field label and encoding for HEAL CDEs\n",
    "    cde_df['Normalized Combined'] = cde_df['Additional Notes (Question Text)'].fillna('') + \" | \" + cde_df['PV Description'].fillna('')\n",
    "    cde_df['Normalized Combined'] = cde_df['Normalized Combined'].apply(normalize_string)\n",
    "    \n",
    "    # Compare each study combined field label and encoding with CDE encodings\n",
    "    for idx, row in study_df.iterrows():\n",
    "        # Skip row if 'Normalized Combined' is empty (i.e., either encoding or field label was blank or NaN)\n",
    "        if row['Normalized Combined'] == '':\n",
    "            continue  # Skip this row and move to the next one\n",
    "        \n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        best_crf_name = None\n",
    "\n",
    "        potential_matches = []\n",
    "        \n",
    "        for _, cde_row in cde_df.iterrows():\n",
    "            # Calculate the Levenshtein similarity score as a percentage\n",
    "            score_percentage = levenshtein_to_percentage(row['Normalized Combined'], cde_row['Normalized Combined'])\n",
    "            \n",
    "            # Find the best match (highest percentage)\n",
    "            if score_percentage > best_score:\n",
    "                best_match = cde_row['Variable Name']\n",
    "                best_score = score_percentage\n",
    "                best_crf_name = cde_row['CRF Name']\n",
    "            # Store all potential matches (we will sort later)\n",
    "            potential_matches.append((cde_row['Variable Name'], score_percentage, cde_row['CRF Name']))\n",
    "        \n",
    "        # Sort potential matches by score percentage in descending order and take the top 3\n",
    "        potential_matches.sort(key=lambda x: x[1], reverse=True)  # Sort by score (index 1)\n",
    "        \n",
    "        # Remove duplicates from the potential matches list by creating a set of unique CDE names\n",
    "        unique_matches = []\n",
    "        seen = set()\n",
    "        for name, score, crf in potential_matches:\n",
    "            if name not in seen:\n",
    "                unique_matches.append((name, score, crf))\n",
    "                seen.add(name)\n",
    "        potential_matches = unique_matches\n",
    "        \n",
    "        # Assign best match and CRF Name\n",
    "        if best_match:\n",
    "            study_df.at[idx, 'Best Match CDE Name'] = best_match\n",
    "            study_df.at[idx, 'Best Match Score'] = best_score\n",
    "            study_df.at[idx, 'Best Match CRF Name'] = best_crf_name  \n",
    "        \n",
    "        # Now the top 3 unique potential matches\n",
    "        for i, (match_name, match_score, crf_name) in enumerate(potential_matches[:3], start=2):\n",
    "            study_df.at[idx, f'Potential Match {i} - CDE Name'] = match_name\n",
    "            study_df.at[idx, f'Potential Match {i} - Score'] = match_score\n",
    "            study_df.at[idx, f'Potential Match {i} - CRF Name'] = crf_name\n",
    "    \n",
    "    # Save the results\n",
    "    output_file = f\"{study_file.rsplit('.', 1)[0]}_vlmd_cdesearch.xlsx\"\n",
    "    study_df.to_excel(output_file, index=False)\n",
    "    print(f\"Comparison complete. Results saved to {output_file}.\")\n",
    "    return study_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed764f84",
   "metadata": {},
   "source": [
    "### Compare Study Encodings with HEAL CDEs\n",
    "\n",
    "This function `compare_encodings()` automates the comparison between a study's variable list and the HEAL Common Data Elements (CDEs).\n",
    "\n",
    "It:\n",
    "- Cleans and normalizes encoding + question text.\n",
    "- Uses Levenshtein distance to find the most similar HEAL CDE for each study variable.\n",
    "- Captures the best match and two alternative potential matches.\n",
    "- Saves results to a new Excel file with the best CDE recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43e3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison complete. Results saved to ./in/SAMPLE_sprint_2020-12-16_vlmd_cdesearch.xlsx.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study_file = './in/SAMPLE_sprint_2020-12-16.xlsx'  # Path to the study file inside the 'in' folder\n",
    "    study_sheet = 'SheetA'  # Replace with the name of the sheet you want to process\n",
    "    encoding_column = 'Choices, Calculations, OR Slider Labels'  # Replace with the correct column name for the PV Description\n",
    "    field_label_column = 'Field Label'  # Replace with the correct column name for the field labels in the study data dictionary\n",
    "    compare_encodings(\n",
    "        study_file,\n",
    "        encoding_column=encoding_column,\n",
    "        field_label_column=field_label_column,\n",
    "        study_sheet=study_sheet\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fc82e9",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "\n",
    "The following block shows an example of how to use the `compare_encodings()` function:\n",
    "\n",
    "- **study_file**: Path to the Excel or CSV file containing the study's data dictionary.\n",
    "- **study_sheet**: The name of the sheet inside the Excel file (only used if input is .xlsx).\n",
    "- **encoding_column**: The column in the study file where the permissible values (encodings) are located.\n",
    "- **field_label_column**: The column in the study file where the field labels (question text) are located.\n",
    "\n",
    "Running the script will output a new Excel file listing the best CDE matches and potential alternatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
