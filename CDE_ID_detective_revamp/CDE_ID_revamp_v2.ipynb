{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import asyncio\n",
    "import os\n",
    "import pandas as pd  # For data handling, like reading from Excel\n",
    "from openai import AsyncOpenAI  # Asynchronous client from the new OpenAI SDK\n",
    "import json\n",
    "import configparser  # For reading configuration files\n",
    "import re\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Retrieve file paths and column names from config\n",
    "input_file = config['Files']['input_file']\n",
    "input_worksheet = config['Files']['input_worksheet']\n",
    "crf_column = config['Columns']['crf_column']\n",
    "variable_column = config['Columns']['variable_column']\n",
    "description_column = config['Columns']['description_column']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "openai_api_key = config['OpenAI']['api_key']\n",
    "client = AsyncOpenAI(api_key=openai_api_key)# Retrieve OpenAI settings from the config file\n",
    "\n",
    "assistant_id = config['OpenAI']['assistant_id']\n",
    "matching_instruction = config['Instructions']['matching_instruction']\n",
    "crf_id = config['Instructions']['crf_id']\n",
    "file_id = config['OpenAI']['file_id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data Dictionary:\n",
      "               module             name  \\\n",
      "0  member_information   participant_id   \n",
      "1  member_information    hcs01_version   \n",
      "2  member_information  cbs3_mi_version   \n",
      "3  member_information      survey_mode   \n",
      "4  member_information      hcs01_lam05   \n",
      "\n",
      "                                         description  \n",
      "0                                     Participant ID  \n",
      "1                                          Version #  \n",
      "2  SMS - Coalition Baseline Survey - Round 3 - CB...  \n",
      "3               How will the survey be administered?  \n",
      "4                                      Contact State  \n",
      "\n",
      "Grouped CRF Names with Descriptions:\n",
      "                                  CRF Name  \\\n",
      "0  hcs_community_coalition_round_3_consent   \n",
      "1   hcs_community_coalition_round_3_survey   \n",
      "2                       member_information   \n",
      "\n",
      "                                        Descriptions  \n",
      "0  Version # : Signature of research participant ...  \n",
      "1  Version # Date of consent DEMOGRAPHIC CHARACTE...  \n",
      "2  Participant ID Version # SMS - Coalition Basel...  \n"
     ]
    }
   ],
   "source": [
    "# Load the data dictionary from Excel file\n",
    "data_dict_df = pd.read_excel(input_file, sheet_name=input_worksheet)\n",
    "\n",
    "# Select only the relevant columns\n",
    "data_dict_df = data_dict_df[[crf_column, variable_column, description_column]]\n",
    "\n",
    "# Display the first few rows of the loaded data\n",
    "print(\"Loaded Data Dictionary:\")\n",
    "print(data_dict_df.head())\n",
    "\n",
    "# Group descriptions by each unique CRF\n",
    "# Concatenate all descriptions for each CRF into a single string\n",
    "grouped_crf_df = data_dict_df.groupby(crf_column)[description_column].apply(lambda x: ' '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "grouped_crf_df.columns = ['CRF Name', 'Descriptions']\n",
    "\n",
    "# Display the grouped DataFrame\n",
    "print(\"\\nGrouped CRF Names with Descriptions:\")\n",
    "print(grouped_crf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 of 2 in API call\n",
    "\n",
    "def parse_extracted_crf_name(response_content):\n",
    "    \"\"\"\n",
    "    Parse the response to extract the full CRF name, including descriptors like \"Short Form\" or abbreviations in parentheses.\n",
    "    \"\"\"\n",
    "    # Debug: Print the full response content\n",
    "    print(\"\\n--- Full Response Content ---\\n\", response_content, \"\\n--- End of Response ---\\n\")\n",
    "\n",
    "    # Modify the regex to capture the entire CRF name, including phrases like \"Short Form\"\n",
    "    crf_name_pattern = r\"(?i)CRF name\\s*:\\s*([\\w\\s-]+(?:\\(.+?\\))?)\"\n",
    "    crf_match = re.search(crf_name_pattern, response_content)\n",
    "    matched_crf = re.sub(r'-\\s*Rationale$', '', crf_match.group(1).strip()) if crf_match else \"Unknown CRF\"\n",
    "\n",
    "    # Extract rationale if available\n",
    "    rationale_pattern = r\"(?i)Rationale\\s*:\\s*(.+)\"\n",
    "    rationale_match = re.search(rationale_pattern, response_content)\n",
    "    rationale = rationale_match.group(1).strip() if rationale_match else \"No rationale provided\"\n",
    "\n",
    "    return {\n",
    "        \"Extracted CRF Name\": matched_crf,\n",
    "        \"Rationale\": rationale,\n",
    "        \"Response\": response_content  # Full response for reference\n",
    "    }\n",
    "\n",
    "async def extract_crf_name(client, crf_name, descriptions):\n",
    "    prompt = f\"{crf_id}\\n\\nCRF Name: {crf_name}\\nDescriptions: {descriptions}\"\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    return parse_extracted_crf_name(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 of 2 in API call\n",
    "\n",
    "def parse_heal_crf_match(response_content):\n",
    "    \"\"\"\n",
    "    Parse the response to extract a matched HEAL Core CRF name and confidence level.\n",
    "    This is used for Step 2.\n",
    "    \"\"\"\n",
    "    # Extract HEAL Core CRF match\n",
    "    crf_match = re.search(r\"(?i)HEAL Core CRF Match:\\s*(.+?)(?=\\s*-|$)\", response_content, re.DOTALL)\n",
    "    matched_crf = crf_match.group(1).strip() if crf_match else \"No CRF match\"\n",
    "\n",
    "    # Extract confidence level\n",
    "    confidence_match = re.search(r\"(?i)Confidence\\s*[Ll]evel:\\s*(High Confidence|Medium Confidence|Low Confidence|No Match)\", response_content)\n",
    "    confidence_level = confidence_match.group(1).strip() if confidence_match else \"No Confidence Score\"\n",
    "\n",
    "    return {\n",
    "        \"Matched CRF\": matched_crf,\n",
    "        \"Confidence\": confidence_level,\n",
    "        \"Response\": response_content  # Full response for reference\n",
    "    }\n",
    "\n",
    "# Async function to call OpenAI API with the new version syntax\n",
    "async def get_crf_match_from_openai(client, extracted_crf_name, descriptions):\n",
    "    prompt = f\"{matching_instruction}\\n\\nCRF Name: {extracted_crf_name}\\nDescriptions: {descriptions}\"\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    return parse_heal_crf_match(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous function to process all CRFs concurrently\n",
    "async def analyze_crfs(client, crf_df):\n",
    "    \"\"\"\n",
    "    Processes each CRF and gathers results asynchronously in two steps.\n",
    "    Step 1: Extract the general CRF name.\n",
    "    Step 2: Match the extracted name to a HEAL Core CRF.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "\n",
    "    # Step 1: Extract CRF names\n",
    "    for _, row in crf_df.iterrows():\n",
    "        crf_name = row['CRF Name']\n",
    "        descriptions = row['Descriptions']\n",
    "        # First step: Extract CRF name (regardless of HEAL Core)\n",
    "        task = extract_crf_name(client, crf_name, descriptions)\n",
    "        tasks.append(task)\n",
    "\n",
    "    # Await extraction results\n",
    "    extracted_names = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Step 2: Match extracted CRF names with HEAL Core CRFs\n",
    "    match_tasks = []\n",
    "    for idx, extracted in enumerate(extracted_names):\n",
    "        extracted_crf_name = extracted[\"Extracted CRF Name\"]\n",
    "        descriptions = crf_df.iloc[idx][\"Descriptions\"]\n",
    "        # Second step: Match the extracted CRF name with HEAL Core CRFs\n",
    "        task = get_crf_match_from_openai(client, extracted_crf_name, descriptions)\n",
    "        match_tasks.append(task)\n",
    "\n",
    "    # Await matching results\n",
    "    matches = await asyncio.gather(*match_tasks)\n",
    "\n",
    "    # Format the results\n",
    "    results = []\n",
    "    for idx, (extracted, match) in enumerate(zip(extracted_names, matches)):\n",
    "        crf_name = crf_df.iloc[idx][\"CRF Name\"]\n",
    "        results.append({\n",
    "            \"Original CRF Name\": crf_name,\n",
    "            \"Extracted CRF Name\": extracted[\"Extracted CRF Name\"],\n",
    "            \"Rationale\": extracted[\"Rationale\"],  # Replace confidence with rationale\n",
    "            \"Full Response (Extracted CRF)\": extracted[\"Response\"],  # Store full response of extraction\n",
    "            \"Matched HEAL Core CRF\": match.get(\"Matched CRF\", \"No Match\"),\n",
    "            \"Match Confidence\": match.get(\"Confidence\", \"No Confidence\"),\n",
    "            \"Match Full Response\": match.get(\"Response\")  # Store full response of matching\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data Dictionary:\n",
      "               module             name  \\\n",
      "0  member_information   participant_id   \n",
      "1  member_information    hcs01_version   \n",
      "2  member_information  cbs3_mi_version   \n",
      "3  member_information      survey_mode   \n",
      "4  member_information      hcs01_lam05   \n",
      "\n",
      "                                         description  \n",
      "0                                     Participant ID  \n",
      "1                                          Version #  \n",
      "2  SMS - Coalition Baseline Survey - Round 3 - CB...  \n",
      "3               How will the survey be administered?  \n",
      "4                                      Contact State  \n",
      "\n",
      "Grouped CRF Names with Descriptions:\n",
      "                                  CRF Name  \\\n",
      "0  hcs_community_coalition_round_3_consent   \n",
      "1   hcs_community_coalition_round_3_survey   \n",
      "2                       member_information   \n",
      "\n",
      "                                        Descriptions  \n",
      "0  Version # : Signature of research participant ...  \n",
      "1  Version # Date of consent DEMOGRAPHIC CHARACTE...  \n",
      "2  Participant ID Version # SMS - Coalition Basel...  \n",
      "\n",
      "--- Full Response Content ---\n",
      " - CRF name: hcs_community_coalition_round_3_consent\n",
      "- Rationale: The description provided outlines a consent form used to obtain informed consent from research participants, including their agreement to participate in the study and to be re-contacted for future surveys. The specific elements such as signatures, printed names, and consent questions align with the typical structure of a consent form in research settings. \n",
      "--- End of Response ---\n",
      "\n",
      "\n",
      "--- Full Response Content ---\n",
      " - CRF name: Coalition Baseline Survey - Round 3 (CBS3)\n",
      "- Rationale: The description includes specific references to a baseline survey (CBS3), participant information, contact details, and survey administration methods, all of which are typical components of case report forms used in survey research. The mention of \"HCS community\" further indicates that this is a structured questionnaire aimed at gathering demographic and community-related data for analysis. \n",
      "--- End of Response ---\n",
      "\n",
      "\n",
      "--- Full Response Content ---\n",
      " - CRF name: hcs_community_coalition_round_3_survey\n",
      "- Rationale: The provided description contains detailed questions and sections related to demographic characteristics, coalition membership, community sector representation, and specific aspects of opioid-related issues, which align closely with the content of the \"hcs_community_coalition_round_3_survey\". This survey focuses on gathering insights from community coalition members regarding their experiences and perceptions related to opioid use and treatment in their communities. \n",
      "--- End of Response ---\n",
      "\n",
      "Results saved to out\\HDP00360_CoalitionBaselineSurveyCBS3202_DataDictionary_2023-08-22_enhanced_2025-03-20.xlsx with two sheets: 'Metadata' and 'EnhancedDD'.\n"
     ]
    }
   ],
   "source": [
    "# Main function to execute the analysis\n",
    "async def main():\n",
    "    # Step 1: Fancy Pivot Table Output\n",
    "\n",
    "    # Load the data dictionary from Excel file\n",
    "    data_dict_df = pd.read_excel(input_file, sheet_name=input_worksheet)\n",
    "    \n",
    "    # Select only the relevant columns\n",
    "    data_dict_df = data_dict_df[[crf_column, variable_column, description_column]]\n",
    "    \n",
    "    # Display the first few rows of the loaded data for confirmation\n",
    "    print(\"Loaded Data Dictionary:\")\n",
    "    print(data_dict_df.head())\n",
    "\n",
    "    # Group descriptions by each unique CRF and concatenate descriptions\n",
    "    grouped_crf_df = data_dict_df.groupby(crf_column)[description_column].apply(lambda x: ' '.join(x.dropna())).reset_index()\n",
    "    grouped_crf_df.columns = ['CRF Name', 'Descriptions']\n",
    "    \n",
    "    # Display grouped data to verify itâ€™s ready for analysis\n",
    "    print(\"\\nGrouped CRF Names with Descriptions:\")\n",
    "    print(grouped_crf_df.head())\n",
    "    \n",
    "    # Run the two-step analyze_crfs process and get the results\n",
    "    results_df = await analyze_crfs(client, grouped_crf_df)\n",
    "\n",
    "    # Step 2: Create the Enhanced Output File\n",
    "\n",
    "    # Load the input file again to ensure all original columns are included\n",
    "    full_input_df = pd.read_excel(input_file, sheet_name=input_worksheet)\n",
    "\n",
    "    # Merge the Input File with the Results File on the Original CRF Name (Form Name)\n",
    "    enhanced_output_df = full_input_df.copy()  # Start with the original input file\n",
    "    enhanced_output_df = enhanced_output_df.merge(\n",
    "        results_df[['Original CRF Name', 'Extracted CRF Name', 'Matched HEAL Core CRF', 'Match Confidence']],\n",
    "        how='left',  # Preserve all rows from the input file\n",
    "        left_on=crf_column,  # Match using the original CRF Name column from the input file\n",
    "        right_on='Original CRF Name'  # Match to the Results File column\n",
    "    )\n",
    "\n",
    "    # Optional: Reorganize the new columns to appear next to the Form Name\n",
    "    form_name_col_index = list(enhanced_output_df.columns).index(crf_column)  # Find the index of the Form Name column\n",
    "    for new_col in ['Extracted CRF Name', 'Matched HEAL Core CRF', 'Match Confidence']:\n",
    "        # Move each new column to the right of the Form Name column\n",
    "        col_data = enhanced_output_df.pop(new_col)\n",
    "        enhanced_output_df.insert(form_name_col_index + 1, new_col, col_data)\n",
    "        form_name_col_index += 1  # Adjust index for the next new column\n",
    "\n",
    "    # Save everything in a **single** Excel file with two sheets\n",
    "    output_file = config['Files']['output_file']\n",
    "    \n",
    "    with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "        results_df.to_excel(writer, sheet_name=\"Metadata\", index=False)  # First sheet\n",
    "        enhanced_output_df.to_excel(writer, sheet_name=\"EnhancedDD\", index=False)  # Second sheet\n",
    "\n",
    "    print(f\"Results saved to {output_file} with two sheets: 'Metadata' and 'EnhancedDD'.\")\n",
    "\n",
    "# Run the main function in a Jupyter-compatible way\n",
    "await main()  # Place this at the end to kick off execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
