{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39bb156d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m  \u001b[38;5;66;03m# For data handling, like reading from Excel\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncOpenAI  \u001b[38;5;66;03m# Asynchronous client from the new OpenAI SDK\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrapidfuzz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fuzz\n",
      "File \u001b[1;32mc:\\Users\\lmbfo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\__init__.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[1;32mc:\\Users\\lmbfo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\compat\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     is_numpy_dev,\n\u001b[0;32m     20\u001b[0m     np_version_under1p21,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[0;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mc:\\Users\\lmbfo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\compat\\numpy\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[0;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[1;32mc:\\Users\\lmbfo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\util\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Appender,\n\u001b[0;32m      4\u001b[0m     Substitution,\n\u001b[0;32m      5\u001b[0m     cache_readonly,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     hash_array,\n\u001b[0;32m     10\u001b[0m     hash_pandas_object,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name):\n",
      "File \u001b[1;32mc:\\Users\\lmbfo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\util\\_decorators.py:14\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     Any,\n\u001b[0;32m      8\u001b[0m     Callable,\n\u001b[0;32m      9\u001b[0m     Mapping,\n\u001b[0;32m     10\u001b[0m     cast,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     F,\n\u001b[0;32m     17\u001b[0m     T,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[1;32mc:\\Users\\lmbfo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\_libs\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     NaT,\n\u001b[0;32m     16\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     iNaT,\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lmbfo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\_libs\\interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import asyncio\n",
    "import os\n",
    "import pandas as pd  # For data handling, like reading from Excel\n",
    "from openai import AsyncOpenAI  # Asynchronous client from the new OpenAI SDK\n",
    "from rapidfuzz import fuzz\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import configparser  # For reading configuration files\n",
    "import re\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71463177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env variables\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "assistant_id     = os.getenv(\"ASSISTANT_ID\")\n",
    "healmatch_id     = os.getenv(\"HEALMATCH_ID\")\n",
    "harmonizer_id    = os.getenv(\"HARMONIZER_ID\")\n",
    "\n",
    "# sanity check (optional)\n",
    "if not all([openai_api_key, assistant_id, healmatch_id, harmonizer_id]):\n",
    "    raise RuntimeError(\"Missing one or more OpenAI env vars\")\n",
    "\n",
    "client = AsyncOpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config_prestep.ini')\n",
    "\n",
    "# Retrieve file paths and column names from config\n",
    "input_file = config['Files']['input_file']\n",
    "input_worksheet = config['Files']['input_worksheet']\n",
    "crf_column = config['Columns']['crf_column']\n",
    "variable_column = config['Columns']['variable_column']\n",
    "description_column = config['Columns']['description_column']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set assistant instructions\n",
    "crf_id = config['Instructions']['crf_id_prestep']\n",
    "matching_instruction = config['Instructions']['matching_instruction']\n",
    "form_harmonizer_prompt = config['Instructions']['form_harmonizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data dictionary from Excel file\n",
    "data_dict_df = pd.read_excel(input_file, sheet_name=input_worksheet)\n",
    "\n",
    "# Select only the relevant columns\n",
    "data_dict_df = data_dict_df[[crf_column, variable_column, description_column]]\n",
    "\n",
    "# Display the first few rows of the loaded data\n",
    "print(\"Loaded Data Dictionary:\")\n",
    "print(data_dict_df.head())\n",
    "\n",
    "# Convert the DataFrame into record dicts\n",
    "records = data_dict_df.to_dict(orient='records')\n",
    "\n",
    "# Take the first record and build the payload\n",
    "first_row = records[0]\n",
    "payload = {\n",
    "    \"crf_name\": first_row[crf_column],\n",
    "    \"variable_name\": first_row[variable_column],\n",
    "    \"description\": first_row[description_column]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dee4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_llm_json(full, crf_name):\n",
    "    try:\n",
    "        data = json.loads(full)\n",
    "        refined = data.get(\"crf_name\", crf_name).strip()\n",
    "        rationale = data.get(\"rationale\", \"\").strip()\n",
    "        return refined, rationale, full\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\n[ERROR] JSON decode error. Full response was:\\n\", full)\n",
    "        return crf_name, \"\", full\n",
    "\n",
    "def normalize_name(name):\n",
    "    # Lowercase, remove punctuation, and drop generic words like \"form\"\n",
    "    name = name.lower()\n",
    "    name = re.sub(r'form|log|assessment|information|status', '', name)  # Remove extra generic words\n",
    "    name = re.sub(r'[^a-z0-9\\s]', '', name)  # Remove punctuation\n",
    "    name = re.sub(r'\\s+', ' ', name)         # Collapse whitespace\n",
    "    return name.strip()\n",
    "\n",
    "def auto_cluster_names(names, threshold=70):\n",
    "    \"\"\"\n",
    "    Clusters similar names using fuzzy matching and assigns the most frequent as canonical.\n",
    "    Returns a dict: {original_name: canonical_name}\n",
    "    \"\"\"\n",
    "    clusters = []\n",
    "    mapping = {}\n",
    "    name_counts = pd.Series(names).value_counts().to_dict()\n",
    "    normalized_names = {name: normalize_name(name) for name in names}\n",
    "    for name in names:\n",
    "        found_cluster = False\n",
    "        for cluster in clusters:\n",
    "            if any(fuzz.ratio(normalized_names[name], normalized_names[c]) >= threshold for c in cluster):\n",
    "                cluster.append(name)\n",
    "                found_cluster = True\n",
    "                break\n",
    "        if not found_cluster:\n",
    "            clusters.append([name])\n",
    "    for cluster in clusters:\n",
    "        canonical = pd.Series(cluster).map(lambda x: name_counts.get(x, 0)).idxmax()\n",
    "        for name in cluster:\n",
    "            mapping[name] = canonical\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper set 1 API call\n",
    "async def refine_crf_name_with_variables(client, variable_names, crf_name, descriptions):\n",
    "    \"\"\"\n",
    "    Calls OpenAI to refine/formulate a unique, concise CRF name \n",
    "    based on the original CRF name, variable names, and descriptions.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"{crf_id}\\n\\n\"\n",
    "        \"Please respond in JSON with keys \"\n",
    "        \"`crf_name` and `rationale` only. Do not wrap in markdown.\\n\\n\"\n",
    "        f\"Variable names: {variable_names}\\n\"\n",
    "        f\"Original form name: {crf_name}\\n\"\n",
    "        f\"Descriptions: {descriptions}\"\n",
    "    )\n",
    "\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    full = response.choices[0].message.content.strip()\n",
    "    print(\"\\n--- Full Prestep Response ---\\n\", full, \"\\n--- End ---\\n\")\n",
    "\n",
    "    # parse the JSON\n",
    "    return parse_llm_json(full, crf_name)\n",
    "\n",
    "async def refine_with_retry(client, var, crf, desc, tries=5):\n",
    "    backoff = 1\n",
    "    for attempt in range(1, tries + 1):\n",
    "        try:\n",
    "            return await refine_crf_name_with_variables(client, var, crf, desc)\n",
    "        except Exception as e:\n",
    "            msg = str(e).lower()\n",
    "            is_rate = (\"rate limit\" in msg) or (\"429\" in msg) \\\n",
    "                      or (hasattr(e, \"code\") and e.code == \"rate_limit_exceeded\")\n",
    "\n",
    "            if is_rate:\n",
    "                print(f\"[rate limit] prestep attempt {attempt}, sleeping {backoff}s\")\n",
    "            else:\n",
    "                print(f\"[warning] prestep failed attempt {attempt}: {e}\")\n",
    "\n",
    "            if attempt == tries:\n",
    "                return crf, \"\", \"\"\n",
    "\n",
    "            await asyncio.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "#loop call row-by-row, run prestep\n",
    "async def run_prestep(client, df, chunk_size=50):\n",
    "    all_r, all_rat, all_full = [], [], []\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start:start+chunk_size]\n",
    "        tasks = [\n",
    "            refine_with_retry(\n",
    "                client,\n",
    "                row[variable_column],\n",
    "                row[crf_column],\n",
    "                row[description_column]\n",
    "            )\n",
    "            for _, row in chunk.iterrows()\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        names, rats, fulls = zip(*results)\n",
    "        all_r.extend(names)\n",
    "        all_rat.extend(rats)\n",
    "        all_full.extend(fulls)\n",
    "        # slight pause between chunks to smooth out rate\n",
    "        await asyncio.sleep(1)\n",
    "    df[\"Refined CRF Name\"], df[\"Rationale\"], df[\"Full Response\"] = all_r, all_rat, all_full\n",
    "    return df\n",
    "\n",
    "async def main():\n",
    "    # … your config loading and DataFrame setup …\n",
    "\n",
    "    # Step 1: refine each CRF name (this loops internally)\n",
    "    refined_df = await run_prestep(client, data_dict_df)\n",
    "\n",
    "    # Now refined_df has a new \"Refined CRF Name\" column.\n",
    "    print(\"After prestep, here’s a sample:\")\n",
    "    print(refined_df[[crf_column, \"Refined CRF Name\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define the function schema\n",
    "harmonize_function = {\n",
    "    \"name\": \"harmonize_crf_names\",\n",
    "    \"description\": \"Map each original CRF name to a single harmonized label\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"mapping\": {\n",
    "                \"type\": \"object\",\n",
    "                \"description\": \"Keys are the original form names; values are the harmonized labels\",\n",
    "                \"additionalProperties\": { \"type\": \"string\" }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"mapping\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2) Harmonizer helper with debug prints and fallbacks\n",
    "def batcher(seq, size=20):\n",
    "    \"\"\"Yield successive size-sized chunks from seq.\"\"\"\n",
    "    for pos in range(0, len(seq), size):\n",
    "        yield seq[pos:pos + size]\n",
    "\n",
    "async def harmonize_crf_names_step(client, refined_df, batch_size=20):\n",
    "    # Build and dedupe the payload\n",
    "    seen = set()\n",
    "    unique_entries = []\n",
    "    for orig, rat in zip(refined_df[\"Refined CRF Name\"], refined_df[\"Rationale\"]):\n",
    "        key = (orig, rat)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_entries.append({\"original\": orig, \"rationale\": rat})\n",
    "\n",
    "    if not unique_entries:\n",
    "        refined_df[\"Canonical CRF Name\"] = refined_df[\"Refined CRF Name\"]\n",
    "        print(\"[Harmonizer] No entries to harmonize, using identity mapping.\")\n",
    "        return refined_df\n",
    "\n",
    "    combined_mapping = {}\n",
    "\n",
    "    # Batch and harmonize!\n",
    "    for batch_num, batch in enumerate(batcher(unique_entries, size=batch_size), start=1):\n",
    "        print(f\"\\n[Harmonizer] Sending batch {batch_num} of {len(batch)}:\")\n",
    "        for e in batch:\n",
    "            print(\"   \", e)\n",
    "\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": config[\"Instructions\"][\"form_harmonizer\"]},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(batch)}\n",
    "            ],\n",
    "            functions=[harmonize_function],\n",
    "            function_call={\"name\": \"harmonize_crf_names\"},\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        choice = response.choices[0].message\n",
    "        print(\"\\n[Harmonizer] Raw model message:\")\n",
    "        print(choice)\n",
    "\n",
    "        # Verify the function name\n",
    "        if choice.function_call:\n",
    "            print(f\"[Harmonizer] Function called: {choice.function_call.name}\")\n",
    "        else:\n",
    "            print(\"[Harmonizer] No function_call detected\")\n",
    "\n",
    "        # Parse the returned arguments\n",
    "        mapping = {}\n",
    "        if choice.function_call:\n",
    "            raw_args = choice.function_call.arguments\n",
    "            print(\"[Harmonizer] Raw function_call.arguments:\", raw_args)\n",
    "            try:\n",
    "                args = json.loads(raw_args)\n",
    "                if \"mapping\" in args and isinstance(args[\"mapping\"], dict):\n",
    "                    mapping = args[\"mapping\"]\n",
    "                else:\n",
    "                    mapping = args\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"[Harmonizer] JSON decode error:\", e)\n",
    "\n",
    "        # Fallback to identity if mapping is empty for this batch\n",
    "        if not mapping:\n",
    "            print(\"[Harmonizer] Empty mapping for this batch; defaulting to identity.\")\n",
    "            mapping = {e['original']: e['original'] for e in batch}\n",
    "\n",
    "        print(\"\\n[Harmonizer] Parsed mapping (original → harmonized):\")\n",
    "        for orig, canon in mapping.items():\n",
    "            print(f\"    '{orig}' -> '{canon}'\")\n",
    "\n",
    "        # Combine batch mapping into all mappings\n",
    "        combined_mapping.update(mapping)\n",
    "\n",
    "    # Apply the combined mapping\n",
    "    refined_df[\"Canonical CRF Name\"] = refined_df[\"Refined CRF Name\"].map(lambda x: combined_mapping.get(x, x))\n",
    "\n",
    "    for col in [\"Refined CRF Name\", \"Canonical CRF Name\"]:\n",
    "        refined_df[col] = refined_df[col].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    print(\"\\n[Harmonizer] Final Canonical CRF Name results:\")\n",
    "    print(\n",
    "        refined_df[[\"Refined CRF Name\", \"Canonical CRF Name\"]]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Auto-cluster for final canonical name\n",
    "    print(\"\\n[Auto-Cluster] Clustering Canonical CRF Names for final deduplication...\")\n",
    "\n",
    "    unique_canonicals = refined_df[\"Canonical CRF Name\"].unique()\n",
    "    auto_map = auto_cluster_names(unique_canonicals, threshold=70)\n",
    "\n",
    "    refined_df[\"Final Canonical CRF Name\"] = refined_df[\"Canonical CRF Name\"].map(auto_map)\n",
    "\n",
    "    print(\"\\n[Auto-Cluster] Clustered Canonical CRF Names:\")\n",
    "    print(refined_df[[\"Canonical CRF Name\", \"Final Canonical CRF Name\"]].drop_duplicates().reset_index(drop=True))\n",
    "\n",
    "    return refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper set 3 API call\n",
    "async def match_heal_core_crf(client, full_prestep_response):\n",
    "    # build a user message that includes the JSON‐only instruction\n",
    "    user_content = (\n",
    "        f\"Prestep output:\\n{full_prestep_response}\\n\\n\"\n",
    "        \"Please respond in strict JSON with keys \"\n",
    "        \"\\\"heal_core_crf\\\", \\\"confidence\\\", and \\\"rationale\\\". \"\n",
    "        \"Do not wrap in markdown or add any extra fields.\"\n",
    "    )\n",
    "\n",
    "    resp = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": matching_instruction},\n",
    "            {\"role\":   \"user\", \"content\": user_content}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    full = resp.choices[0].message.content.strip()\n",
    "    print(\"\\n--- HEAL-Match Response ---\\n\", full, \"\\n--- End ---\\n\")\n",
    "\n",
    "    # now json.loads should actually work\n",
    "    data = json.loads(full)\n",
    "    match     = data.get(\"heal_core_crf\",    \"No CRF match\").strip()\n",
    "    conf      = data.get(\"confidence\",       \"Low Confidence\").strip()\n",
    "    rationale = data.get(\"rationale\",        \"\").strip()\n",
    "    return match, conf, rationale\n",
    "\n",
    "async def match_with_retry(client, full_response, tries=5):\n",
    "    backoff = 1\n",
    "    for attempt in range(1, tries + 1):\n",
    "        try:\n",
    "            return await match_heal_core_crf(client, full_response)\n",
    "        except Exception as e:\n",
    "            msg = str(e).lower()\n",
    "            is_rate = (\n",
    "                \"rate limit\" in msg or\n",
    "                \"429\" in msg or\n",
    "                (hasattr(e, \"code\") and e.code == \"rate_limit_exceeded\")\n",
    "            )\n",
    "\n",
    "            if is_rate:\n",
    "                print(f\"[rate limit] match attempt {attempt}, sleeping {backoff}s\")\n",
    "            else:\n",
    "                print(f\"[warning] match failed attempt {attempt}: {e}\")\n",
    "\n",
    "            if attempt == tries:\n",
    "                return \"No CRF match\", \"Low Confidence\", \"\"\n",
    "\n",
    "            await asyncio.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "# Loop over prestep outputs\n",
    "async def run_heal_match(client, df, chunk_size=50):\n",
    "    all_match, all_conf, all_mrat = [], [], []\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start:start+chunk_size]\n",
    "        tasks = [\n",
    "            match_with_retry(client, row[\"Full Response\"])\n",
    "            for _, row in chunk.iterrows()\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        matches, confs, mrats = zip(*results)\n",
    "        all_match.extend(matches)\n",
    "        all_conf.extend(confs)\n",
    "        all_mrat.extend(mrats)\n",
    "        await asyncio.sleep(1)\n",
    "    df[\"HEAL Core CRF Match\"], df[\"Confidence Level\"], df[\"Match Rationale\"] = all_match, all_conf, all_mrat\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestrator\n",
    "async def main():\n",
    "    # Load the full input file (all original columns)\n",
    "    full_input_df = pd.read_excel(input_file, sheet_name=input_worksheet)\n",
    "\n",
    "    # Extract just the columns we need for prestep\n",
    "    data_dict_df = full_input_df[[crf_column, variable_column, description_column]].copy()\n",
    "\n",
    "    # Prestep: get Refined CRF Name, Rationale, Full Response\n",
    "    refined_df = await run_prestep(client, data_dict_df, chunk_size=50)\n",
    "\n",
    "    # Harmonize the refined names via your new assistant\n",
    "    refined_df = await harmonize_crf_names_step(client, refined_df, batch_size=20)\n",
    "\n",
    "    # Merge prestep outputs back into the full DataFrame, using Canonical\n",
    "    enhanced_df = full_input_df.join(\n",
    "        refined_df[[\"Canonical CRF Name\", \"Rationale\", \"Full Response\"]]\n",
    "    )\n",
    "\n",
    "    # HEAL-Core matching: adds three new columns\n",
    "    final_df = await run_heal_match(client, enhanced_df, chunk_size=50)\n",
    "\n",
    "    # --- Harmonize Confidence Level for No CRF match ---\n",
    "    # Replace any 'Confidence Level' with 'No CRF match' where 'HEAL Core CRF Match' is 'No CRF match'\n",
    "    mask = final_df[\"HEAL Core CRF Match\"] == \"No CRF match\"\n",
    "    final_df.loc[mask, \"Confidence Level\"] = \"No CRF match\"\n",
    "\n",
    "    # Build a *unique* metadata table:\n",
    "    metadata_df = (\n",
    "        refined_df\n",
    "        [[crf_column, \"Canonical CRF Name\", \"Rationale\", \"Full Response\"]]\n",
    "        .drop_duplicates(subset=[crf_column, \"Canonical CRF Name\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Save to Excel:\n",
    "    output_file = config[\"Files\"][\"output_file\"]\n",
    "    with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "        # Metadata sheet: one row per (section, refined CRF) combo\n",
    "        metadata_df.to_excel(writer, sheet_name=\"Metadata\", index=False)\n",
    "        # EnhancedDD sheet: the full original + all new columns\n",
    "        final_df.to_excel(writer, sheet_name=\"EnhancedDD\", index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_file} with sheets 'Metadata' and 'EnhancedDD'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    final_df = asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
