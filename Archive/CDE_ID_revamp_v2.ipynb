{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import asyncio\n",
    "import os\n",
    "import pandas as pd  # For data handling, like reading from Excel\n",
    "from openai import AsyncOpenAI  # Asynchronous client from the new OpenAI SDK\n",
    "import json\n",
    "import configparser  # For reading configuration files\n",
    "import re\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Retrieve file paths and column names from config\n",
    "input_file = config['Files']['input_file']\n",
    "input_worksheet = config['Files']['input_worksheet']\n",
    "crf_column = config['Columns']['crf_column']\n",
    "variable_column = config['Columns']['variable_column']\n",
    "description_column = config['Columns']['description_column']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "openai_api_key = config['OpenAI']['api_key']\n",
    "client = AsyncOpenAI(api_key=openai_api_key)# Retrieve OpenAI settings from the config file\n",
    "\n",
    "assistant_id = config['OpenAI']['assistant_id']\n",
    "crf_id = config['Instructions']['crf_id']\n",
    "matching_instruction = config['Instructions']['matching_instruction']\n",
    "file_id = config['OpenAI']['file_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data Dictionary:\n",
      "  Refined CRF Name              name  \\\n",
      "0    Screening Log         record_id   \n",
      "1    Screening Log         subjectid   \n",
      "2    Screening Log        oboe_group   \n",
      "3    Screening Log  study_identifier   \n",
      "4    Screening Log       sc_birthdat   \n",
      "\n",
      "                                         description  \n",
      "0                                          Record ID  \n",
      "1                                  OBOE Study Number  \n",
      "2          What group was the subject enrolled into?  \n",
      "3  Study Identifier (hidden, raw value from oboe ...  \n",
      "4                                      Date of Birth  \n",
      "\n",
      "Grouped CRF Names with Descriptions and Variable Names:\n",
      "                        CRF Name  \\\n",
      "0          Extensive Travel Form   \n",
      "1       Maternal Medical History   \n",
      "2  Preauthorization Request Form   \n",
      "3                  Screening Log   \n",
      "4      Travel Authorization Form   \n",
      "\n",
      "                                        Descriptions  \\\n",
      "0  SECTION C. FORM COMPLETION: 1. Initials of sta...   \n",
      "1  SECTION D. MATERNAL/FETAL EXPOSURE (CHECK ALL ...   \n",
      "2  SECTION B. REQUEST DECISION: If YES, Date of p...   \n",
      "3  Record ID OBOE Study Number What group was the...   \n",
      "4  SECTION B. REQUEST DECISION: 2. Amount approve...   \n",
      "\n",
      "                                      Variable Names  \n",
      "0                                         etcompinit  \n",
      "1  opiodrx___1, opiodrx___2, opiodrx___3, opiodrx...  \n",
      "2                                         preauthdat  \n",
      "3  record_id, subjectid, oboe_group, study_identi...  \n",
      "4                                       amountapprov  \n"
     ]
    }
   ],
   "source": [
    "# Load the data dictionary from Excel file\n",
    "data_dict_df = pd.read_excel(input_file, sheet_name=input_worksheet)\n",
    "\n",
    "# Select only the relevant columns\n",
    "data_dict_df = data_dict_df[[crf_column, variable_column, description_column]]\n",
    "\n",
    "# Display the first few rows of the loaded data\n",
    "print(\"Loaded Data Dictionary:\")\n",
    "print(data_dict_df.head())\n",
    "\n",
    "# Group descriptions by each unique CRF\n",
    "# Concatenate all descriptions for each CRF into a single string\n",
    "grouped_descriptions_df = (\n",
    "    data_dict_df.groupby(crf_column)[description_column]\n",
    "    .apply(lambda x: ' '.join(x.dropna()))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Group variable names by each unique CRF\n",
    "# Concatenate all variable names for each CRF into a comma-separated string\n",
    "grouped_variables_df = (\n",
    "    data_dict_df.groupby(crf_column)[variable_column]\n",
    "    .apply(lambda x: ', '.join(x.dropna()))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge grouped descriptions and variable names on CRF Name\n",
    "grouped_crf_df = grouped_descriptions_df.merge(\n",
    "    grouped_variables_df, on=crf_column, how='left'\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "grouped_crf_df.columns = ['CRF Name', 'Descriptions', 'Variable Names']\n",
    "\n",
    "# Display the grouped DataFrame with descriptions and variables\n",
    "print(\"\\nGrouped CRF Names with Descriptions and Variable Names:\")\n",
    "print(grouped_crf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 of 2 in API call\n",
    "\n",
    "def parse_extracted_crf_name(response_content):\n",
    "    \"\"\"\n",
    "    Parse the response to extract the full CRF name, including descriptors like \"Short Form\" or abbreviations in parentheses.\n",
    "    \"\"\"\n",
    "    # Debug: Print the full response content\n",
    "    print(\"\\n--- Full Response Content ---\\n\", response_content, \"\\n--- End of Response ---\\n\")\n",
    "\n",
    "    # Modify the regex to capture the entire CRF name, including phrases like \"Short Form\"\n",
    "    crf_name_pattern = r\"(?i)CRF name\\s*:\\s*([\\w\\s-]+(?:\\(.+?\\))?)\"\n",
    "    crf_match = re.search(crf_name_pattern, response_content)\n",
    "    matched_crf = re.sub(r'-\\s*Rationale$', '', crf_match.group(1).strip()) if crf_match else \"Unknown CRF\"\n",
    "\n",
    "    # Extract rationale if available\n",
    "    rationale_pattern = r\"(?i)Rationale\\s*:\\s*(.+)\"\n",
    "    rationale_match = re.search(rationale_pattern, response_content)\n",
    "    rationale = rationale_match.group(1).strip() if rationale_match else \"No rationale provided\"\n",
    "\n",
    "    return {\n",
    "        \"Extracted CRF Name\": matched_crf,\n",
    "        \"Rationale\": rationale,\n",
    "        \"Response\": response_content  # Full response for reference\n",
    "    }\n",
    "\n",
    "async def extract_crf_name(client, crf_name, descriptions):\n",
    "    prompt = f\"{crf_id}\\n\\nCRF Name: {crf_name}\\nDescriptions: {descriptions}\"\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    return parse_extracted_crf_name(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 of 2 in API call\n",
    "\n",
    "def parse_heal_crf_match(response_content):\n",
    "    \"\"\"\n",
    "    Parse the response to extract a matched HEAL Core CRF name and confidence level.\n",
    "    This is used for Step 2.\n",
    "    \"\"\"\n",
    "    # Extract HEAL Core CRF match\n",
    "    crf_match = re.search(r\"(?i)HEAL Core CRF Match:\\s*(.+?)(?=\\s*-|$)\", response_content, re.DOTALL)\n",
    "    matched_crf = crf_match.group(1).strip() if crf_match else \"No CRF match\"\n",
    "\n",
    "    # Extract confidence level\n",
    "    confidence_match = re.search(r\"(?i)Confidence\\s*[Ll]evel:\\s*(High Confidence|Medium Confidence|Low Confidence|No Match)\", response_content)\n",
    "    confidence_level = confidence_match.group(1).strip() if confidence_match else \"No Confidence Score\"\n",
    "\n",
    "    return {\n",
    "        \"Matched CRF\": matched_crf,\n",
    "        \"Confidence\": confidence_level,\n",
    "        \"Response\": response_content  # Full response for reference\n",
    "    }\n",
    "\n",
    "# Async function to call OpenAI API with the new version syntax\n",
    "async def get_crf_match_from_openai(client, extracted_crf_name, descriptions):\n",
    "    prompt = f\"{matching_instruction}\\n\\nCRF Name: {extracted_crf_name}\\nDescriptions: {descriptions}\"\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    return parse_heal_crf_match(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous function to process all CRFs concurrently\n",
    "async def analyze_crfs(client, crf_df, extracted_names):\n",
    "    # Match extracted CRF names with HEAL Core CRFs\n",
    "    step2_tasks = []\n",
    "    for idx, extracted in enumerate(extracted_names):\n",
    "        extracted_crf_name = extracted[\"Extracted CRF Name\"]\n",
    "        descriptions = crf_df.iloc[idx][\"Descriptions\"]\n",
    "        task = get_crf_match_from_openai(client, extracted_crf_name, descriptions)\n",
    "        step2_tasks.append(task)\n",
    "\n",
    "    matches = await asyncio.gather(*step2_tasks)\n",
    "\n",
    "    # Format and return results as before\n",
    "    results = []\n",
    "    for idx, (extracted, match) in enumerate(zip(extracted_names, matches)):\n",
    "        original_crf_name = crf_df.iloc[idx][\"CRF Name\"]\n",
    "        results.append({\n",
    "            \"Original CRF Name\": original_crf_name,\n",
    "            \"Refined CRF Name\": crf_df.iloc[idx][\"Refined CRF Name\"],\n",
    "            \"Extracted CRF Name\": extracted[\"Extracted CRF Name\"],\n",
    "            \"Rationale\": extracted[\"Rationale\"],\n",
    "            \"Full Response (Extracted CRF)\": extracted[\"Response\"],\n",
    "            \"Matched HEAL Core CRF\": match.get(\"Matched CRF\", \"No Match\"),\n",
    "            \"Match Confidence\": match.get(\"Confidence\", \"No Confidence\"),\n",
    "            \"Match Full Response\": match.get(\"Response\")\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data Dictionary:\n",
      "  Refined CRF Name              name  \\\n",
      "0    Screening Log         record_id   \n",
      "1    Screening Log         subjectid   \n",
      "2    Screening Log        oboe_group   \n",
      "3    Screening Log  study_identifier   \n",
      "4    Screening Log       sc_birthdat   \n",
      "\n",
      "                                         description  \n",
      "0                                          Record ID  \n",
      "1                                  OBOE Study Number  \n",
      "2          What group was the subject enrolled into?  \n",
      "3  Study Identifier (hidden, raw value from oboe ...  \n",
      "4                                      Date of Birth  \n",
      "\n",
      "Grouped CRF Names with Descriptions and Variable Names:\n",
      "                        CRF Name  \\\n",
      "0          Extensive Travel Form   \n",
      "1       Maternal Medical History   \n",
      "2  Preauthorization Request Form   \n",
      "3                  Screening Log   \n",
      "4      Travel Authorization Form   \n",
      "\n",
      "                                        Descriptions  \\\n",
      "0  SECTION C. FORM COMPLETION: 1. Initials of sta...   \n",
      "1  SECTION D. MATERNAL/FETAL EXPOSURE (CHECK ALL ...   \n",
      "2  SECTION B. REQUEST DECISION: If YES, Date of p...   \n",
      "3  Record ID OBOE Study Number What group was the...   \n",
      "4  SECTION B. REQUEST DECISION: 2. Amount approve...   \n",
      "\n",
      "                                      Variable Names  \n",
      "0                                         etcompinit  \n",
      "1  opiodrx___1, opiodrx___2, opiodrx___3, opiodrx...  \n",
      "2                                         preauthdat  \n",
      "3  record_id, subjectid, oboe_group, study_identi...  \n",
      "4                                       amountapprov  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "analyze_crfs() missing 1 required positional argument: 'extracted_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with two sheets: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetadata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnhancedDD\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Run the main function in a Jupyter-compatible way\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main()  \u001b[38;5;66;03m# Place this at the end to kick off execution\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 42\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(grouped_crf_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Run the two-step analyze_crfs process and get the results\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43manalyze_crfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouped_crf_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Debug: Print columns in results_df before merge\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mColumns in results_df:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: analyze_crfs() missing 1 required positional argument: 'extracted_names'"
     ]
    }
   ],
   "source": [
    "# Main function to execute the analysis\n",
    "async def main():\n",
    "    # Step 1: Fancy Pivot Table Output\n",
    "\n",
    "    # Load the data dictionary from Excel file\n",
    "    data_dict_df = pd.read_excel(input_file, sheet_name=input_worksheet)\n",
    "    \n",
    "    # Select only the relevant columns\n",
    "    data_dict_df = data_dict_df[[crf_column, variable_column, description_column]]\n",
    "    \n",
    "    # Display the first few rows of the loaded data for confirmation\n",
    "    print(\"Loaded Data Dictionary:\")\n",
    "    print(data_dict_df.head())\n",
    "\n",
    "    # Group descriptions by each unique CRF and concatenate descriptions\n",
    "    grouped_descriptions_df = (\n",
    "        data_dict_df.groupby(crf_column)[description_column]\n",
    "        .apply(lambda x: ' '.join(x.dropna()))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Group variable names by each unique CRF and concatenate variable names\n",
    "    grouped_variables_df = (\n",
    "        data_dict_df.groupby(crf_column)[variable_column]\n",
    "        .apply(lambda x: ', '.join(x.dropna()))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge descriptions and variable names into one DataFrame\n",
    "    grouped_crf_df = grouped_descriptions_df.merge(\n",
    "        grouped_variables_df, on=crf_column, how='left'\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    grouped_crf_df.columns = ['CRF Name', 'Descriptions', 'Variable Names']\n",
    "    \n",
    "    # Display grouped data to verify it’s ready for analysis\n",
    "    print(\"\\nGrouped CRF Names with Descriptions and Variable Names:\")\n",
    "    print(grouped_crf_df.head())\n",
    "    \n",
    "    # Run the two-step analyze_crfs process and get the results\n",
    "    results_df = await analyze_crfs(client, grouped_crf_df)\n",
    "\n",
    "    # Debug: Print columns in results_df before merge\n",
    "    print(\"\\nColumns in results_df:\")\n",
    "    print(results_df.columns.tolist())\n",
    "\n",
    "    # Step 2: Create the Enhanced Output File\n",
    "\n",
    "    # Load the input file again to ensure all original columns are included\n",
    "    full_input_df = pd.read_excel(input_file, sheet_name=input_worksheet)\n",
    "\n",
    "    # Merge the Input File with the Results File on the Original CRF Name (Form Name)\n",
    "    enhanced_output_df = full_input_df.copy()  # Start with the original input file\n",
    "    enhanced_output_df = enhanced_output_df.merge(\n",
    "        results_df[['Original CRF Name', 'Extracted CRF Name', 'Matched HEAL Core CRF', 'Match Confidence']],\n",
    "        how='left',  # Preserve all rows from the input file\n",
    "        left_on=crf_column,  # Match using the original CRF Name column from the input file\n",
    "        right_on='Original CRF Name'  # Match to the Results File column\n",
    "    )\n",
    "\n",
    "    # Debug: Print columns in enhanced_output_df after merge\n",
    "    print(\"\\nColumns in enhanced_output_df after merge:\")\n",
    "    print(enhanced_output_df.columns.tolist())\n",
    "\n",
    "    # Optional: Reorganize the new columns to appear next to the Form Name column\n",
    "    form_name_col_index = list(enhanced_output_df.columns).index(crf_column)  # Find the index of the Form Name column\n",
    "    for new_col in ['Extracted CRF Name', 'Matched HEAL Core CRF', 'Match Confidence']:\n",
    "        if new_col in enhanced_output_df.columns:\n",
    "            # Move each new column to the right of the Form Name column\n",
    "            col_data = enhanced_output_df.pop(new_col)\n",
    "            enhanced_output_df.insert(form_name_col_index + 1, new_col, col_data)\n",
    "            form_name_col_index += 1  # Adjust index for the next new column\n",
    "        else:\n",
    "            print(f\"Warning: Column '{new_col}' not found in enhanced_output_df. Skipping.\")\n",
    "\n",
    "\n",
    "    # Save everything in a **single** Excel file with two sheets\n",
    "    output_file = config['Files']['output_file']\n",
    "    \n",
    "    with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "        results_df.to_excel(writer, sheet_name=\"Metadata\", index=False)  # First sheet\n",
    "        enhanced_output_df.to_excel(writer, sheet_name=\"EnhancedDD\", index=False)  # Second sheet\n",
    "\n",
    "    print(f\"Results saved to {output_file} with two sheets: 'Metadata' and 'EnhancedDD'.\")\n",
    "\n",
    "# Run the main function in a Jupyter-compatible way\n",
    "await main()  # Place this at the end to kick off execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
